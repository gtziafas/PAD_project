<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video">
  <meta property="og:title" content="Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video"/>
  <meta property="og:description" content="'Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video' project website, 2025."/>
  <meta property="og:url" content="https://gtziafas.github.io/PAD_project/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video">
  <meta name="twitter:description" content="'Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video' project website, 2025">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video</h1>    
	     <!-- Paper authors -->
             <!-- <div class="is-size-4 publication-authors">
    		    <span class="author-block" style="color: darkred;">Conference on Robot Learning 2024</span>
              </div>	   -->
  
	      <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/gtziafas" target="_blank">Georgios Tziafas </a>,</span>
                <span class="author-block">
                  Jiayun Zhang </a>,</span>
                <span class="author-block">
                  <a href="https://hkasaei.github.io/" target="_blank">Hamidreza Kasaei</a>,</span>
                  </div>

                  <div class="is-size-8 publication-authors">
                    <span class="author-block">
                      <a href="https://www.ai.rug.nl/irl-lab/" target="_blank">Interactive Robot Learning Lab</a><br>Department of Artificial Intelligence, University of Groningen</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Paper link -->
                    <span class="link-block">
                      <a href="static/pdfs/PAD_paper_compressed.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="gtziafas.github.io/PAD_project/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- YouTube video Link -->
               <span class="link-block">
                  <a href="gtziafas.github.io/PAD_project/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
               	  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
	       </span>


                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="gtziafas.github.io/PAD_project/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
	

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning visuomotor policies from expert demonstrations is an important frontier in modern robotics research, however, most popular methods require copious efforts for collecting teleoperation data and struggle to generalize out-of-distribution. Scaling data collection has been explored through leveraging human videos, as well as demonstration augmentation techniques. The latter approach typically requires expensive simulation rollouts and trains policies with synthetic image data, therefore introducing a sim-to-real gap. In parallel, alternative state representations such as keypoints have shown great promise for category-level generalization. In this work, we bring these avenues together in a unified framework: PAD (Parse-Augment-Distill), for learning generalizable bimanual policies from a single human video. Our method relies on three steps: (a) parsing a human video demo into a robot-executable keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to augment the demonstration at scale without simulators, and (c) distilling the augmented trajectories into a keypoint-conditioned policy. Empirically, we showcase that PAD outperforms state-of-the-art bimanual demonstration augmentation works relying on image policies with simulation rollouts, both in terms of success rate and sample/cost efficiency.
            We deploy our framework in six diverse real-world bimanual tasks such as pouring drinks, cleaning trash and opening containers, producing one-shot policies that generalize in unseen spatial arrangements, object instances and background distractors.            
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Parse-Augment-Distill Framework</h2>
      <div class="columns is-centered has-text-centered">
       <div class="item">
        <!-- Your image here -->
          <img src="static/images/fig1.drawio.png" alt="alt text" style="margin-top: 20px;" />
          <h2 class="subtitle has-text-justified" style="font-size: 1rem; margin-bottom: 25px;">
            Given one human video demonstration, PAD executes three subsequent steps: (a) parsing the video into a robot-executable state-action trajectory, (b) spatially augmenting the demo at scale via bimanual TAMP, and (c) distilling the generated data into a closed-loop keypoint policy. The obtained policy can generalize to unseen spatial arrangements, object instances and background scene noise.
        </h2>
      </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method image -->


<!-- Parse image -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Parse</h2>
      <div class="columns is-centered has-text-centered">
       <div class="item">
        <!-- Your image here -->
          <img src="static/images/Parse.drawio-1.png" alt="alt text" style="margin-top: 20px; width: 90%; height: auto;" />
          <h2 class="subtitle has-text-justified" style="font-size: 1rem; margin-bottom: 25px;">
        Parsing source video into a robot-executable keypoint-action trajectory. Keypoints are annotated manually and tracked over frames with a point-tracker. End-effector actions are produced by mapping hand points, estimated by a hand pose estimator, to 6-dof gripper actions. 
        </h2>
      </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End parse image --

<!-- Augment image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper images. -->
      <h2 class="title is-3 has-text-centered">Augment</h2>
      <div class="columns is-centered has-text-centered">
        
        <!-- First image -->
        <div class="column">
          <img src="static/images/PAD_Augm_Templates.drawio.png" alt="Image 1" style="width: 100%; height: auto; margin-top: 20px;" />
          <p class="has-text-centered" style="font-size: 0.8rem; margin-top: 5px;">
            <!-- Caption for Image 1 --> Grounding trajectory segments with bimanual task templates. The video is abstracted into a symbolic template denoting task stages, hand-object assignments and requirement for synchronization. For each arm-stage exact timetsamps in the demo are grounded into motion (orange), idle (gray), asynchronous skill (blue) and synchronous skill (green) segments. 
          </p>
        </div>

        <!-- Second image -->
        <div class="column">
          <img src="static/images/PAD_Augment_Transforms.drawio.png" alt="Image 2" style="width: 100%; height: auto; margin-top: 20px;" />
          <p class="has-text-centered" style="font-size: 0.8rem; margin-top: 5px;">
            <!-- Caption for Image 2 --> Augmenting keypoint-action trajectories with SE(3)-equivariant transforms. EE actions are produced such that the relative pose between robot-object remains the same. Keypoint tracks are produced assuming that they move rigidly together with the EE, preserving their relative pose during grasping.
          </p>
        </div>

        <!-- Third image -->
        <div class="column">
          <img src="static/images/PAD_Augment_Bi.drawio.png" alt="Image 3" style="width: 100%; height: auto; margin-top: 20px;" />
          <p class="has-text-centered" style="font-size: 0.8rem; margin-top: 5px;">
            <!-- Caption for Image 3 -->Handling bimanual issues during augmentation, such as novel hand-object assignements (top) and re-synchronization between arms before synchronous stages (bottom). 
          </p>
        </div>

      </div>

      <!-- Content section -->
       <div class="content has-text-centered">
          <p> Augment demo trajectory with bimanual TAMP.
          </p>
        </div>
      <div class="content has-text-justified">
        <p>Our spatial augmentation framework is:</p>
        <ul>
          <li><strong>general</strong>, as it doesn't depend on object semantics and doesn't require skill labels.</li>
          <li><strong>simulation-free</strong>, as it doesn't require simulation rollouts with digital twins and significantly boosts collection time (1000 demos under 1min.).</li>
          <li><strong>view-invariant</strong>, as generated data are expressed wrt. a calibrated frame, independent of downstream camera placement.</li>
          <li><strong>bimanual-aware</strong>, as it handles bimanual issues such as out-of-range hand-object assignments and arm de-synchronization.</li>
          <li><strong>embodiment-agnostic</strong>, as all planning is done in EE space, any robot morphology and motion planner can be plugged in.</li>
        </ul>
      </div>

    </div>
  </div>
</section>
<!-- End augment image -->

<!-- Distill image -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper images. -->
      <h2 class="title is-3 has-text-centered">Distill</h2>
      <div class="columns is-centered has-text-centered">
        
        <!-- First image -->
        <div class="column">
          <img src="static/images/kprdt.drawio.png" alt="Image 1" style="width: 80%; height: auto; margin-top: 20px;" />
          <p class="has-text-centered" style="font-size: 0.8rem; margin-top: 5px;">
            <!-- Caption for Image 1 --> Kp-RDT architecture
          </p>
        </div>

        <!-- Second image -->
        <div class="column">
          <img src="static/images/kp-rollout.gif" alt="Image 2" style="width: 70%; height: auto; margin-top: 20px;" />
          <p class="has-text-centered" style="font-size: 0.8rem; margin-top: 5px;">
            <!-- Caption for Image 2 --> Example rollout from robot-view
        </p>
        </div>

      </div>

        <!-- Content section -->
        <div class="content has-text-centered">
          <p> Distill generated data into a closed-loop, keypoint-conditioned diffusion policy.
          </p>
        </div>

      </div>
    </div>
  </section>
  <!-- End distill image -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Generalizable One-Shot Bimanual Policies</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/pour_drink.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/trash_in_bin.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/empty_vase.mp4"
            type="video/mp4">
         </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/open_container.mp4"
            type="video/mp4">
         </video>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/mix_bowl.mp4"
            type="video/mp4">
         </video>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/paper_in_case.mp4"
            type="video/mp4">
         </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{tziafas2024openworldgraspinglargevisionlanguage,
      title={Towards Open-World Grasping with Large Vision-Language Models}, 
      author={Georgios Tziafas and Hamidreza Kasaei},
      year={2024},
      journal={8th Conference on Robot Learning (CoRL 2024)},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
